{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense,Embedding,GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RNN, LSTM, RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "87\n"
     ]
    }
   ],
   "source": [
    "char2idx = {'\\n': 0, '\\r': 1, ' ': 2, '!': 3, '$': 4, '%': 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '?': 25, '@': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, '[': 53, ']': 54, '`': 55, 'a': 56, 'b': 57, 'c': 58, 'd': 59, 'e': 60, 'f': 61, 'g': 62, 'h': 63, 'i': 64, 'j': 65, 'k': 66, 'l': 67, 'm': 68, 'n': 69, 'o': 70, 'p': 71, 'q': 72, 'r': 73, 's': 74, 't': 75, 'u': 76, 'v': 77, 'w': 78, 'x': 79, 'y': 80, 'z': 81, '‘': 82, '’': 83, '“': 84, '”': 85, '\\ufeff': 86}\n",
    "idx2char = ['\\n', '\\r', ' ', '!', '$', '%' ,'(', ')', '*', ',', '-', '.', '/', \n",
    "            '0' ,'1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@',\n",
    "            'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I','J', 'K', 'L', 'M', 'N', 'O',\n",
    "            'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '`',\n",
    "            'a', 'b' ,'c' ,'d', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p',\n",
    "            'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n",
    "print(len(char2idx))\n",
    "print(len(idx2char))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_text(sample):\n",
    "    return ''.join([idx2char[int(x)] for x in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            22272     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 87)             89175     \n",
      "=================================================================\n",
      "Total params: 5,358,423\n",
      "Trainable params: 5,358,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(batch_size=64):\n",
    "    vocab_size = len(char2idx)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(vocab_size, 256, batch_input_shape=[batch_size, None]))\n",
    "    model.add(LSTM(1024, return_sequences=True,\n",
    "                        stateful=True,#!!!\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    return model  \n",
    "        \n",
    "model = build_model(1)\n",
    "model.summary()\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss) #loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('weights_4000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Mowgli laire know you don’t. I’m not talking to you. You don’t know what blood\n",
      "is.”\n",
      "\n",
      "“When Bamagain.\n",
      "\n",
      "Mowgli laid his hands on Baloo and Bagheera to get them away, and the\n",
      "two great beasts stathe blish in a beat fuss about it.\n",
      "\n",
      "“I’ve no nursery to fight for,” said Kotick. “I only want to show you\n",
      "all a plowg four-year-old holluschickie\n",
      "romped down from Hutchinson’s Hill crying: “Out of the way, youngsteremember that I huntid Mowgli, and he bounded away.\n",
      "\n",
      "“That is a man. That is all a man,” \n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, size=1000,temperature=1.,verbose=0):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "    print('Text:',start_string,end='')\n",
    "\n",
    "    # Convert the  start_string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    " \n",
    "    # Empty string to store the results\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(size):\n",
    "        if(verbose): print(\"====================================================\")\n",
    "        if(verbose): print('input:',to_text(input_eval.numpy()[0]))\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        if(verbose): print('\"Best\" prediction:',to_text(predictions.numpy().argmax(axis=2)[0]))\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        ## Taking alays the best prediction is NOT a good idea - easy to have a loop \n",
    "        # predicted_id = predictions.numpy().argmax(axis=1)[0]\n",
    "        \n",
    "        # It is better to generate a categorical distribution and take a character fro this distrbution\n",
    "        predictions = predictions * temperature\n",
    "        samples = tf.random.categorical(predictions, num_samples=10)\n",
    "        if(verbose):\n",
    "            print('sampled predictions:')\n",
    "            for j in range(samples.shape[1]):\n",
    "                  print(to_text(samples[:,j].numpy()), end=', ')\n",
    "            print()        \n",
    "            \n",
    "        predicted_id = samples[-1,0].numpy()\n",
    "        #print(\"Predicted_id\",predicted_id)\n",
    "        \n",
    "        if(verbose): print('chosen_id',predicted_id,'letter:',idx2char[predicted_id])\n",
    " \n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        if(verbose):\n",
    "            #print(text_generated)\n",
    "            print(start_string + ''.join(text_generated))\n",
    "        else:\n",
    "            print(idx2char[predicted_id],end='')\n",
    "        \n",
    "    print()    \n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "txt = generate_text(model, start_string=\"Mowgli \",size=500, temperature=1, verbose=0)\n",
    "print()\n",
    "print('='*50)\n",
    "##print('The final generated text:\\n',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Fight just\n",
      "as well have petted Teddy for playing in the dust. Rikki was not fight for,” said Kotick. “I only want to show you\n",
      "all a placiuni, who had been trained under the Law of the Jungle, did not like\n",
      "or understand this kind of life. Toomai the Jungle-People to cross\n",
      "each other’s path. But whenever they found a s reg/fundraising.  Contributions to the Project Gutenberg\n",
      "Literary Archive Foundation are tax deductibli, who had been trained under the Law of their master. Aaa-ssp! We must remind t\n"
     ]
    }
   ],
   "source": [
    "txt = generate_text(model, start_string=\"Fight \",size=500, temperature=1, verbose=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
