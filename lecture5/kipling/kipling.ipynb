{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dense,Embedding,GRU\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import RNN, LSTM, RepeatVector\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open file and prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 unique characters\n",
      "char2idx:\n",
      " {'\\n': 0, '\\r': 1, ' ': 2, '!': 3, '$': 4, '%': 5, '(': 6, ')': 7, '*': 8, ',': 9, '-': 10, '.': 11, '/': 12, '0': 13, '1': 14, '2': 15, '3': 16, '4': 17, '5': 18, '6': 19, '7': 20, '8': 21, '9': 22, ':': 23, ';': 24, '?': 25, '@': 26, 'A': 27, 'B': 28, 'C': 29, 'D': 30, 'E': 31, 'F': 32, 'G': 33, 'H': 34, 'I': 35, 'J': 36, 'K': 37, 'L': 38, 'M': 39, 'N': 40, 'O': 41, 'P': 42, 'Q': 43, 'R': 44, 'S': 45, 'T': 46, 'U': 47, 'V': 48, 'W': 49, 'X': 50, 'Y': 51, 'Z': 52, '[': 53, ']': 54, '`': 55, 'a': 56, 'b': 57, 'c': 58, 'd': 59, 'e': 60, 'f': 61, 'g': 62, 'h': 63, 'i': 64, 'j': 65, 'k': 66, 'l': 67, 'm': 68, 'n': 69, 'o': 70, 'p': 71, 'q': 72, 'r': 73, 's': 74, 't': 75, 'u': 76, 'v': 77, 'w': 78, 'x': 79, 'y': 80, 'z': 81, '‘': 82, '’': 83, '“': 84, '”': 85, '\\ufeff': 86}\n",
      "idx2char\n",
      " ['\\n' '\\r' ' ' '!' '$' '%' '(' ')' '*' ',' '-' '.' '/' '0' '1' '2' '3' '4'\n",
      " '5' '6' '7' '8' '9' ':' ';' '?' '@' 'A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I'\n",
      " 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' '['\n",
      " ']' '`' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p'\n",
      " 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '‘' '’' '“' '”' '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "fin = open('kipling.txt', 'rb')\n",
    "\n",
    "dataset_txt = fin.read().decode(encoding='utf-8')\n",
    "fin.close()\n",
    "\n",
    "# Obtain the unique characters\n",
    "vocab = sorted(set(dataset_txt))\n",
    "print ('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "# Create a mapping from unique characters to indices\n",
    "char2idx = {char:index for index, char in enumerate(vocab)}\n",
    "print('char2idx:\\n',char2idx)\n",
    "idx2char = np.array(vocab)\n",
    "print('idx2char\\n',idx2char)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Convert the dataset from 'characters' to 'integers'\n",
    "dataset_int = np.array([char2idx[char] for char in dataset_txt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_text(sample):\n",
    "    return ''.join([idx2char[int(x)] for x in sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare samples and labels\n",
    "- Every label is the text shifted by one letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up against Mother\n",
      "Wolf, for he knew that where he was she had all the advantage of the\n",
      "ground, and --> [76. 71.  2. 56. 62. 56. 64. 69. 74. 75.  2. 39. 70. 75. 63. 60. 73.  1.\n",
      "  0. 49. 70. 67. 61.  9.  2. 61. 70. 73.  2. 63. 60.  2. 66. 69. 60. 78.\n",
      "  2. 75. 63. 56. 75.  2. 78. 63. 60. 73. 60.  2. 63. 60.  2. 78. 56. 74.\n",
      "  2. 74. 63. 60.  2. 63. 56. 59.  2. 56. 67. 67.  2. 75. 63. 60.  2. 56.\n",
      " 59. 77. 56. 69. 75. 56. 62. 60.  2. 70. 61.  2. 75. 63. 60.  1.  0. 62.\n",
      " 73. 70. 76. 69. 59.  9.  2. 56. 69. 59.]\n",
      "samples[0]:\n",
      "﻿THE JUNGLE BOOK\n",
      "\n",
      "By Rudyard Kipling\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "     Mowgli’s Brothers\n",
      "     Hunting-Song \n",
      "labels[0]:\n",
      "THE JUNGLE BOOK\n",
      "\n",
      "By Rudyard Kipling\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "     Mowgli’s Brothers\n",
      "     Hunting-Song o\n",
      "\n",
      "samples[1]:\n",
      "of the Seeonee Pack\n",
      "     Kaa’s Hunting\n",
      "     Road-Song of the Bandar-Log\n",
      "     “Tiger! Tiger!”\n",
      "   \n",
      "labels[1]:\n",
      "f the Seeonee Pack\n",
      "     Kaa’s Hunting\n",
      "     Road-Song of the Bandar-Log\n",
      "     “Tiger! Tiger!”\n",
      "    \n",
      "\n",
      "samples[2]:\n",
      "   Mowgli’s Song\n",
      "     The White Seal\n",
      "     Lukannon\n",
      "     “Rikki-Tikki-Tavi”\n",
      "      Darzee’s Chant\n",
      "labels[2]:\n",
      "  Mowgli’s Song\n",
      "     The White Seal\n",
      "     Lukannon\n",
      "     “Rikki-Tikki-Tavi”\n",
      "      Darzee’s Chant\n",
      "\n",
      "\n",
      "samples[3]:\n",
      "\n",
      "     Toomai of the Elephants\n",
      "     Shiv and the Grasshopper\n",
      "     Her Majesty’s Servants\n",
      "     Para\n",
      "labels[3]:\n",
      "     Toomai of the Elephants\n",
      "     Shiv and the Grasshopper\n",
      "     Her Majesty’s Servants\n",
      "     Parad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LEN=100\n",
    "samples = []\n",
    "labels = []\n",
    "for i in range(0,len(dataset_int)-LEN,LEN):\n",
    "    samples.append(dataset_int[i:LEN+i])\n",
    "    labels.append(dataset_int[(i+1):(LEN+i+1)])\n",
    "samples = np.array(samples,dtype=float)\n",
    "labels = np.array(labels,dtype=float)\n",
    "print(to_text(samples[101]),'-->',samples[101])\n",
    "\n",
    "for i in range(4):\n",
    "    print('samples[{}]:\\n{}'.format(i,to_text(samples[i])))\n",
    "    print('labels[{}]:\\n{}'.format(i,to_text(labels[i])))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size = 64 - the model expects batches of 64 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           22272     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 87)            89175     \n",
      "=================================================================\n",
      "Total params: 5,358,423\n",
      "Trainable params: 5,358,423\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(batch_size=1):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Embedding(vocab_size, 256, batch_input_shape=[batch_size, None]))\n",
    "    model.add(LSTM(1024, return_sequences=True,\n",
    "                        stateful=True,#!!!\n",
    "                        recurrent_initializer='glorot_uniform'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    return model  \n",
    "        \n",
    "model = build_model(64)\n",
    "model.summary()\n",
    "\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "\n",
    "num_epochs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function that samples *n* random pairs (sample,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample_from_dataset(n,samples,labels):\n",
    "    prev_numbers = []\n",
    "    new_samples = []\n",
    "    new_labels = []\n",
    "    while len(new_samples)<n:\n",
    "        number = random.randrange(len(samples))\n",
    "        if number in prev_numbers: continue\n",
    "        prev_numbers.append(number)\n",
    "        new_samples.append(samples[number])\n",
    "        new_labels.append(labels[number])\n",
    "    new_samples = np.array(new_samples)    \n",
    "    new_labels = np.array(new_labels)\n",
    "    return new_samples,new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model (long process...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running...\n",
      "==================================================\n",
      "EPOCH  5\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.5378\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.2192\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2230\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.2244\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.2093\n",
      "\n",
      "teth,n $e/Hsuosykttr hSt heteshnathgonnnbtfu sraB rTnlneepfshecsa fea,danaehylbra.thcgnae i\n",
      "\n",
      "==================================================\n",
      "EPOCH  10\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.2095\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1791\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1563\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1538\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1574\n",
      "\n",
      "Polsko oe  ?tb\n",
      "ce aCptp lndd phms ue riu ,  idi T jpoob o c eeehslg urtit h ”itiweka fgfus l\n",
      " svytoue,once \n",
      "\n",
      "==================================================\n",
      "EPOCH  15\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1639\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1500\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1396\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1347\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1320\n",
      "\n",
      "aghlao wfaacsudhdro yhoey hisvlahsi gendav thtc -\n",
      "  cf yf gan o.irundeteeatslurrsro cos-sntiat\n",
      "\n",
      "==================================================\n",
      "EPOCH  20\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1414\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.1357\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1275\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1187\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.1105\n",
      "\n",
      "Polsko  beolt nushg\n",
      "dpaaw aua: oBeg t\n",
      "adh ye Viahdi\n",
      "eg a-ab tieht hlsmeeotekiahi ai s njttcn  rho\n",
      "\n",
      "==================================================\n",
      "EPOCH  25\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 998us/step - loss: 3.0905\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0852\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0789\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0718\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 3.0641\n",
      "\n",
      "Polsko ral \n",
      "h crt tltGr\n",
      "awrY  hlat dteiiTreetbo orl fi,ch yoan t Rlhaatasisehls- sai\n",
      "  glldt\n",
      "e\n",
      "\n",
      "==================================================\n",
      "EPOCH  30\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0437\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0367\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0291\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0206\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0110\n",
      "\n",
      " olsko weh  ar -cgcanro rsasholrtCoiihg,sih Boetn fsto atplBya oent erne r),aemqgd haee﻿ce ke nIious.s ot\n",
      "\n",
      "==================================================\n",
      "EPOCH  35\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0677\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0564\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0442\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.0321\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 3.0206\n",
      "\n",
      "au,uuge’r t t iotl twyee8hru fhm df e nhmentdty xst\n",
      "Ide\n",
      "Moage \n",
      "’t l aea nf-\n",
      "ki\n",
      "\n",
      "==================================================\n",
      "EPOCH  40\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9688\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9582\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9453\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.9310\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9163\n",
      "\n",
      "Polsko cogl! wekdte\n",
      "sra. t sio ohPtrlha!\n",
      "\n",
      "l meho  se l oowe cn-, \n",
      "dus lt odotoyd \n",
      "t “el! te don- a .ct,diu\n",
      "\n",
      "==================================================\n",
      "EPOCH  45\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9420\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9263\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.9082\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.8891\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.8711\n",
      "\n",
      "Polsko cecthas n dthered, snepon \n",
      "/ lhaAid lnhe se at temAhdirte i,slBsB\n",
      "ylrghderhi t  ”i ybx itorlahn h h\n",
      "\n",
      "==================================================\n",
      "EPOCH  50\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.8396\n",
      "Epoch 2/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.8200\n",
      "Epoch 3/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7993\n",
      "Epoch 4/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7788\n",
      "Epoch 5/5\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.7586\n",
      "\n",
      "Polsko yu\n",
      "TioBraa t.ccthu r ku. \n",
      "gdabknak oeuoin pm”swl fhc,oinrs.cd toanGd afe the lakurenr ketha\n",
      "\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5 ## much more to get meaningful results...\n",
    "\n",
    "print('running...')\n",
    "for i in range(10): # much more to get meaningful results...\n",
    "    print(50*'=')\n",
    "    print(\"EPOCH \",num_epochs)\n",
    "    print(50*'=')\n",
    "    # randomly choose 64 samples (and labels)\n",
    "    s,l = sample_from_dataset(64,samples,labels)\n",
    "    \n",
    "    # use these samples to train the model in EPOCHS epochs\n",
    "    H = model.fit(s,l,epochs=EPOCHS,verbose=1,batch_size=64)\n",
    "    num_epochs += EPOCHS\n",
    "    print()\n",
    "    \n",
    "    # generate the text using the current model\n",
    "    txt = generate_text(model, start_string=\"Mowgli \",len=100)\n",
    "    print()\n",
    "    # save the model and weights\n",
    "    model.save('models/model_{}.h5'.format(num_epochs))\n",
    "    model.save_weights('weights/weight_{}.h5'.format(num_epochs))\n",
    "print('done!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generator - generates text using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polsko ugo.(n8iooswnam\n",
      "  eld  sg li, ya   i l ilaet  d  iiie l   o i c nd l k Jhyd    r i ls \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, len=1000):\n",
    "    print(start_string,end='')\n",
    "     # Convert the start_string to numbers\n",
    "    input_data = [char2idx[s] for s in start_string]\n",
    "    input_data = tf.expand_dims(input_data, 0)\n",
    "\n",
    "    # Empty string to store the results\n",
    "    text_generated = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(len):\n",
    "        # the model expects batch of 64 samples so we must produce the batch...\n",
    "        input_data_64 = input_data\n",
    "        for i in range(63):\n",
    "            input_data_64 = np.vstack((input_data_64,input_data))\n",
    "        input_data = input_data_64\n",
    "\n",
    "        predictions = model(input_data)\n",
    "        \n",
    "        # we are interested only in the first prediction\n",
    "        predictions = predictions[0]\n",
    "\n",
    "        # it does NOT work - if we always take max it is easy to have a loop!\n",
    "        # predicted_id = predictions.numpy().argmax(axis=1)[0]\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        #predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    " \n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_data = tf.expand_dims([predicted_id], 0)\n",
    "        print(idx2char[predicted_id],end='')\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    print()    \n",
    "    return (start_string + ''.join(text_generated))\n",
    "\n",
    "string = \"Polsko \"\n",
    "txt = generate_text(model, start_string=string,len=100)\n",
    "#print(\"Start string: \",string)\n",
    "#print(\"Generated string:\\n>\",txt+\"<\")\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
