{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 400001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "glove_vectors = 'myglove.6B.50d.txt'\n",
    "filecp = codecs.open(glove_vectors, encoding = 'utf-8')\n",
    "glove = np.loadtxt(filecp, dtype='str', comments=None)\n",
    "# Extract the vectors and words\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "# Create lookup of words to vectors\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "\n",
    "# Create a mapping from unique characters to indices\n",
    "word2idx = {char:index for index, char in enumerate(words)}\n",
    "idx2word = np.array(words)\n",
    "vocab = len(words)\n",
    "print(\"Vocabulary:\",vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugo characters: 3303543 distinct characters: 119\n",
      "dickens characters: 181399 distinct characters: 83\n",
      "wells characters: 361811 distinct characters: 90\n",
      "kipling characters: 298210 distinct characters: 87\n"
     ]
    }
   ],
   "source": [
    "def load_file(filename):\n",
    "    fin = open(filename, 'rb')\n",
    "    txt = fin.read().decode(encoding='utf-8')\n",
    "    fin.close()\n",
    "    return txt\n",
    "\n",
    "book_names = (\"hugo\",\"dickens\",\"wells\",\"kipling\")\n",
    "texts = {}\n",
    "for bn in book_names:\n",
    "    texts[bn] = load_file(bn+'.txt')\n",
    "\n",
    "for k in texts.keys():\n",
    "    print(k, \"characters:\",len(texts[k]),\"distinct characters:\",len(set(texts[k])))\n",
    "\n",
    "num_books = len(texts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_books = len(texts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(txt):\n",
    "    txt = txt.replace('\\r','')\n",
    "    # txt = txt.replace('\\n',' \\n ')\n",
    "    txt = txt.replace('\\n',' ')\n",
    "    txt = txt.replace(',',' ')\n",
    "    txt = txt.replace(';',' ')\n",
    "    txt = txt.replace('.',' ')\n",
    "    txt = txt.replace('(','')\n",
    "    txt = txt.replace(')','')\n",
    "    txt = txt.replace('!',' ')\n",
    "    txt = txt.replace('?',' ')\n",
    "    txt = txt.replace('_',' ')\n",
    "    txt = txt.replace('“','')\n",
    "    txt = txt.replace('„','')\n",
    "    txt = txt.replace('\"\"','')\n",
    "    txt = txt.lower()\n",
    "    for i in range(5):\n",
    "        txt = txt.replace('  ',' ')\n",
    "    return txt\n",
    "\n",
    "# converts a string to list of words\n",
    "def text2words(dataset_txt):\n",
    "    cleaned_dataset_txt = clean_txt(dataset_txt)\n",
    "    words = cleaned_dataset_txt.rstrip().split(' ')\n",
    "    return words\n",
    "\n",
    "def to_text(sample):\n",
    "    return ' '.join([idx2word[int(x)] for x in sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugo words: 573082 distinct words: 27267\n",
      "dickens words: 32247 distinct words: 5152\n",
      "wells words: 63317 distinct words: 7605\n",
      "kipling words: 54907 distinct words: 5711\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for k in texts.keys():\n",
    "    w = text2words(texts[k])\n",
    "    words.append(w)\n",
    "    print(k,\"words:\",len(w),\"distinct words:\",len(set(w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the dataset from 'words' to 'integers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 573082 not known words: 16270 % of unknown words: 0.028390352514997854\n",
      "words: 32247 not known words: 1540 % of unknown words: 0.047756380438490405\n",
      "words: 63317 not known words: 1158 % of unknown words: 0.018288927144368812\n",
      "words: 54907 not known words: 2135 % of unknown words: 0.038883931010617954\n"
     ]
    }
   ],
   "source": [
    "def words2ints(words):\n",
    "    d = []\n",
    "    for word in words:\n",
    "        if word in word2idx:\n",
    "            d.append(word2idx[word])\n",
    "        else:\n",
    "            #print(word)\n",
    "            d.append(0)\n",
    "    return np.array(d)\n",
    "\n",
    "ibooks = []\n",
    "for w in words:\n",
    "    ibooks.append(words2ints(w))\n",
    "\n",
    "for dint in ibooks:\n",
    "    l = len(dint)\n",
    "    l0 = np.count_nonzero(dint==0)\n",
    "    print(\"words:\",l,\"not known words:\",l0, \"% of unknown words:\", l0/l)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: (596, 200, 1)\n",
      "Labels: (596, 4)\n"
     ]
    }
   ],
   "source": [
    "# take MAX word\n",
    "MAX=30000\n",
    "# length of one sample\n",
    "LEN=200\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "for b,book in enumerate(ibooks):\n",
    "    for i in range(0,MAX-LEN,LEN):\n",
    "        samples.append(book[i:LEN+i])\n",
    "        labels.append(b)\n",
    "samples = np.array(samples,dtype=float)\n",
    "samples = np.expand_dims(samples,axis=2)\n",
    "labels = np.array(labels,dtype=float)\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "print(\"Samples:\",samples.shape)\n",
    "print(\"Labels:\",labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace word with the embedding (vector of 50 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with embedding (596, 200, 50)\n"
     ]
    }
   ],
   "source": [
    "samples50 = np.zeros((len(samples),LEN,50))\n",
    "for i in range(len(samples)):\n",
    "    for j in range(LEN):\n",
    "        si = samples[i,j,0]\n",
    "        v = word_lookup[idx2word[int(si)]]\n",
    "        samples50[i,j] = v\n",
    "        \n",
    "print(\"Samples with embedding\",samples50.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1024)              4202496   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,206,596\n",
      "Trainable params: 4,206,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(1024, input_shape=(None,size))) #size is the size of ONE sample\n",
    "    model.add(Dense(num_books,activation='softmax'))\n",
    "    return model  \n",
    "        \n",
    "model1 = build_model(1)\n",
    "model50 = build_model(50)\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model50.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "(trainSamples, testSamples, trainSamples50, testSamples50, trainLabels, testLabels) = sklearn.model_selection.train_test_split(samples, samples50, labels, test_size=0.5, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 298 samples\n",
      "Epoch 1/30\n",
      "298/298 [==============================] - 43s 145ms/sample - loss: 1.5934 - accuracy: 0.2752 - val_loss: 1.6690 - val_accuracy: 0.2383\n",
      "Epoch 2/30\n",
      "298/298 [==============================] - 42s 142ms/sample - loss: 1.4568 - accuracy: 0.3020 - val_loss: 1.4921 - val_accuracy: 0.2349\n",
      "Epoch 3/30\n",
      "298/298 [==============================] - 46s 153ms/sample - loss: 1.3827 - accuracy: 0.2953 - val_loss: 1.4703 - val_accuracy: 0.2685\n",
      "Epoch 4/30\n",
      "298/298 [==============================] - 48s 163ms/sample - loss: 1.3414 - accuracy: 0.3423 - val_loss: 1.5586 - val_accuracy: 0.2517\n",
      "Epoch 5/30\n",
      "298/298 [==============================] - 50s 166ms/sample - loss: 1.3662 - accuracy: 0.3289 - val_loss: 1.4464 - val_accuracy: 0.2550\n",
      "Epoch 6/30\n",
      "298/298 [==============================] - 51s 170ms/sample - loss: 1.2951 - accuracy: 0.3322 - val_loss: 1.4944 - val_accuracy: 0.2416\n",
      "Epoch 7/30\n",
      "298/298 [==============================] - 52s 175ms/sample - loss: 1.2848 - accuracy: 0.3624 - val_loss: 1.4813 - val_accuracy: 0.3020\n",
      "Epoch 8/30\n",
      "298/298 [==============================] - 52s 174ms/sample - loss: 1.2957 - accuracy: 0.3758 - val_loss: 1.4842 - val_accuracy: 0.3054\n",
      "Epoch 9/30\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 1.2401 - accuracy: 0.4262 - val_loss: 1.4918 - val_accuracy: 0.2685\n",
      "Epoch 10/30\n",
      "298/298 [==============================] - 51s 170ms/sample - loss: 1.2052 - accuracy: 0.4362 - val_loss: 1.4722 - val_accuracy: 0.2685\n",
      "Epoch 11/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 1.1701 - accuracy: 0.4664 - val_loss: 1.4878 - val_accuracy: 0.2953\n",
      "Epoch 12/30\n",
      "298/298 [==============================] - 50s 168ms/sample - loss: 1.1370 - accuracy: 0.5134 - val_loss: 1.5453 - val_accuracy: 0.2953\n",
      "Epoch 13/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 1.0945 - accuracy: 0.5134 - val_loss: 1.5377 - val_accuracy: 0.2953\n",
      "Epoch 14/30\n",
      "298/298 [==============================] - 51s 172ms/sample - loss: 1.0481 - accuracy: 0.5235 - val_loss: 1.5981 - val_accuracy: 0.2987\n",
      "Epoch 15/30\n",
      "298/298 [==============================] - 51s 170ms/sample - loss: 0.9336 - accuracy: 0.6174 - val_loss: 1.6892 - val_accuracy: 0.2785\n",
      "Epoch 16/30\n",
      "298/298 [==============================] - 51s 171ms/sample - loss: 0.9218 - accuracy: 0.6040 - val_loss: 1.6441 - val_accuracy: 0.3389\n",
      "Epoch 17/30\n",
      "298/298 [==============================] - 52s 175ms/sample - loss: 0.8714 - accuracy: 0.6409 - val_loss: 1.7636 - val_accuracy: 0.3188\n",
      "Epoch 18/30\n",
      "298/298 [==============================] - 52s 174ms/sample - loss: 0.8084 - accuracy: 0.6946 - val_loss: 1.7719 - val_accuracy: 0.2987\n",
      "Epoch 19/30\n",
      "298/298 [==============================] - 51s 172ms/sample - loss: 0.7480 - accuracy: 0.7483 - val_loss: 1.7926 - val_accuracy: 0.3389\n",
      "Epoch 20/30\n",
      "298/298 [==============================] - 51s 171ms/sample - loss: 0.6730 - accuracy: 0.7282 - val_loss: 1.8567 - val_accuracy: 0.3121\n",
      "Epoch 21/30\n",
      "298/298 [==============================] - 52s 174ms/sample - loss: 0.5816 - accuracy: 0.7987 - val_loss: 2.0202 - val_accuracy: 0.3188\n",
      "Epoch 22/30\n",
      "298/298 [==============================] - 51s 172ms/sample - loss: 0.5519 - accuracy: 0.8356 - val_loss: 1.9657 - val_accuracy: 0.3389\n",
      "Epoch 23/30\n",
      "298/298 [==============================] - 52s 173ms/sample - loss: 0.4875 - accuracy: 0.8423 - val_loss: 2.1321 - val_accuracy: 0.3221\n",
      "Epoch 24/30\n",
      "298/298 [==============================] - 51s 172ms/sample - loss: 0.4073 - accuracy: 0.8893 - val_loss: 2.1400 - val_accuracy: 0.3389\n",
      "Epoch 25/30\n",
      "298/298 [==============================] - 52s 174ms/sample - loss: 0.3252 - accuracy: 0.9295 - val_loss: 2.1924 - val_accuracy: 0.3490\n",
      "Epoch 26/30\n",
      "298/298 [==============================] - 52s 175ms/sample - loss: 0.2831 - accuracy: 0.9195 - val_loss: 2.3199 - val_accuracy: 0.3255\n",
      "Epoch 27/30\n",
      "298/298 [==============================] - 52s 173ms/sample - loss: 0.2522 - accuracy: 0.9362 - val_loss: 2.3298 - val_accuracy: 0.3389\n",
      "Epoch 28/30\n",
      "298/298 [==============================] - 51s 173ms/sample - loss: 0.2333 - accuracy: 0.9362 - val_loss: 2.4241 - val_accuracy: 0.3591\n",
      "Epoch 29/30\n",
      "298/298 [==============================] - 52s 175ms/sample - loss: 0.1647 - accuracy: 0.9732 - val_loss: 2.5143 - val_accuracy: 0.3154\n",
      "Epoch 30/30\n",
      "298/298 [==============================] - 52s 175ms/sample - loss: 0.1503 - accuracy: 0.9765 - val_loss: 2.5510 - val_accuracy: 0.3423\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "H = model1.fit(trainSamples,trainLabels,epochs=EPOCHS,verbose=1,validation_data=(testSamples,testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 298 samples\n",
      "Epoch 1/30\n",
      "298/298 [==============================] - 51s 171ms/sample - loss: 1.4391 - accuracy: 0.3423 - val_loss: 1.1762 - val_accuracy: 0.4866\n",
      "Epoch 2/30\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.1960 - accuracy: 0.5067 - val_loss: 2.2026 - val_accuracy: 0.4094\n",
      "Epoch 3/30\n",
      "298/298 [==============================] - 49s 164ms/sample - loss: 1.3283 - accuracy: 0.5436 - val_loss: 1.2563 - val_accuracy: 0.4262\n",
      "Epoch 4/30\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.1075 - accuracy: 0.5772 - val_loss: 2.3896 - val_accuracy: 0.3356\n",
      "Epoch 5/30\n",
      "298/298 [==============================] - 49s 164ms/sample - loss: 1.3444 - accuracy: 0.5000 - val_loss: 1.1922 - val_accuracy: 0.5067\n",
      "Epoch 6/30\n",
      "298/298 [==============================] - 50s 166ms/sample - loss: 1.1476 - accuracy: 0.5369 - val_loss: 1.2583 - val_accuracy: 0.4396\n",
      "Epoch 7/30\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.0772 - accuracy: 0.5604 - val_loss: 1.2529 - val_accuracy: 0.3960\n",
      "Epoch 8/30\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 1.1094 - accuracy: 0.5705 - val_loss: 1.2137 - val_accuracy: 0.4597\n",
      "Epoch 9/30\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.0177 - accuracy: 0.5973 - val_loss: 1.1533 - val_accuracy: 0.5034\n",
      "Epoch 10/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 1.2884 - accuracy: 0.5503 - val_loss: 1.2546 - val_accuracy: 0.4463\n",
      "Epoch 11/30\n",
      "298/298 [==============================] - 50s 166ms/sample - loss: 1.0868 - accuracy: 0.6141 - val_loss: 1.1957 - val_accuracy: 0.4832\n",
      "Epoch 12/30\n",
      "298/298 [==============================] - 50s 166ms/sample - loss: 0.9758 - accuracy: 0.5906 - val_loss: 1.1553 - val_accuracy: 0.5101\n",
      "Epoch 13/30\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 0.9567 - accuracy: 0.6544 - val_loss: 1.1436 - val_accuracy: 0.5134\n",
      "Epoch 14/30\n",
      "298/298 [==============================] - 50s 166ms/sample - loss: 0.9160 - accuracy: 0.6510 - val_loss: 1.0622 - val_accuracy: 0.5000\n",
      "Epoch 15/30\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 0.8845 - accuracy: 0.6510 - val_loss: 1.6788 - val_accuracy: 0.5101\n",
      "Epoch 16/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 1.0313 - accuracy: 0.6040 - val_loss: 1.2360 - val_accuracy: 0.4497\n",
      "Epoch 17/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 0.8956 - accuracy: 0.6745 - val_loss: 3.5538 - val_accuracy: 0.3557\n",
      "Epoch 18/30\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 1.1481 - accuracy: 0.6342 - val_loss: 1.0499 - val_accuracy: 0.5503\n",
      "Epoch 19/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 0.7785 - accuracy: 0.7047 - val_loss: 1.0362 - val_accuracy: 0.5570\n",
      "Epoch 20/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 0.7873 - accuracy: 0.6879 - val_loss: 1.1109 - val_accuracy: 0.5201\n",
      "Epoch 21/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 0.7214 - accuracy: 0.7584 - val_loss: 1.1354 - val_accuracy: 0.5872\n",
      "Epoch 22/30\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 0.8689 - accuracy: 0.6913 - val_loss: 1.1376 - val_accuracy: 0.5168\n",
      "Epoch 23/30\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 0.7732 - accuracy: 0.7248 - val_loss: 1.0781 - val_accuracy: 0.5168\n",
      "Epoch 24/30\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 0.7775 - accuracy: 0.6879 - val_loss: 1.0189 - val_accuracy: 0.5336\n",
      "Epoch 25/30\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 0.7341 - accuracy: 0.7617 - val_loss: 1.1666 - val_accuracy: 0.5034\n",
      "Epoch 26/30\n",
      "298/298 [==============================] - 50s 166ms/sample - loss: 0.6698 - accuracy: 0.8154 - val_loss: 1.0446 - val_accuracy: 0.5604\n",
      "Epoch 27/30\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 0.6009 - accuracy: 0.7953 - val_loss: 0.9580 - val_accuracy: 0.6007\n",
      "Epoch 28/30\n",
      "298/298 [==============================] - 54s 181ms/sample - loss: 0.5277 - accuracy: 0.8154 - val_loss: 0.9166 - val_accuracy: 0.6141\n",
      "Epoch 29/30\n",
      "298/298 [==============================] - 54s 180ms/sample - loss: 0.4217 - accuracy: 0.8624 - val_loss: 1.0761 - val_accuracy: 0.5805\n",
      "Epoch 30/30\n",
      "298/298 [==============================] - 53s 179ms/sample - loss: 0.4390 - accuracy: 0.8456 - val_loss: 0.9377 - val_accuracy: 0.6007\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=30\n",
    "H = model50.fit(trainSamples50,trainLabels,epochs=EPOCHS,verbose=1,validation_data=(testSamples50,testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,cohen_kappa_score\n",
    "def show_results(model,testSamples,testLabels):\n",
    "    testResults = model.predict(testSamples)\n",
    "    print(confusion_matrix(testLabels.argmax(axis=1), testResults.argmax(axis=1)))\n",
    "    print(classification_report(testLabels.argmax(axis=1), testResults.argmax(axis=1)))\n",
    "    print(\"Cohen's Kappa: {}\".format(cohen_kappa_score(testLabels.argmax(axis=1), testResults.argmax(axis=1))))\n",
    "    print(\"Accuracy: \",accuracy_score(testLabels.argmax(axis=1), testResults.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "('hugo', 'dickens', 'wells', 'kipling')\n",
      "[[16 32 20 12]\n",
      " [13 34 10  9]\n",
      " [14 17 27 17]\n",
      " [12 21 19 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.20      0.24        80\n",
      "           1       0.33      0.52      0.40        66\n",
      "           2       0.36      0.36      0.36        75\n",
      "           3       0.40      0.32      0.36        77\n",
      "\n",
      "    accuracy                           0.34       298\n",
      "   macro avg       0.34      0.35      0.34       298\n",
      "weighted avg       0.34      0.34      0.33       298\n",
      "\n",
      "Cohen's Kappa: 0.12809565749600682\n",
      "Accuracy:  0.3422818791946309\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1\")\n",
    "print(book_names)\n",
    "show_results(model1,testSamples,testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 50\n",
      "[[57  9  7  7]\n",
      " [13 24 13 16]\n",
      " [10  9 45 11]\n",
      " [ 2 12 10 53]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.71      0.70        80\n",
      "           1       0.44      0.36      0.40        66\n",
      "           2       0.60      0.60      0.60        75\n",
      "           3       0.61      0.69      0.65        77\n",
      "\n",
      "    accuracy                           0.60       298\n",
      "   macro avg       0.59      0.59      0.59       298\n",
      "weighted avg       0.59      0.60      0.60       298\n",
      "\n",
      "Cohen's Kappa: 0.4655796009403822\n",
      "Accuracy:  0.6006711409395973\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 50\")\n",
    "show_results(model50,testSamples50,testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model95_30k\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
