{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 400001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "glove_vectors = 'myglove.6B.50d.txt'\n",
    "filecp = codecs.open(glove_vectors, encoding = 'utf-8')\n",
    "glove = np.loadtxt(filecp, dtype='str', comments=None)\n",
    "# Extract the vectors and words\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "# Create lookup of words to vectors\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "\n",
    "# Create a mapping from unique characters to indices\n",
    "word2idx = {char:index for index, char in enumerate(words)}\n",
    "idx2word = np.array(words)\n",
    "vocab = len(words)\n",
    "print(\"Vocabulary:\",vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugo characters: 3303543 distinct characters: 119\n",
      "dickens characters: 181399 distinct characters: 83\n",
      "wells characters: 361811 distinct characters: 90\n",
      "kipling characters: 298210 distinct characters: 87\n"
     ]
    }
   ],
   "source": [
    "def load_file(filename):\n",
    "    fin = open(filename, 'rb')\n",
    "    txt = fin.read().decode(encoding='utf-8')\n",
    "    fin.close()\n",
    "    return txt\n",
    "\n",
    "book_names = (\"hugo\",\"dickens\",\"wells\",\"kipling\")\n",
    "texts = {}\n",
    "for bn in book_names:\n",
    "    texts[bn] = load_file(bn+'.txt')\n",
    "\n",
    "for k in texts.keys():\n",
    "    print(k, \"characters:\",len(texts[k]),\"distinct characters:\",len(set(texts[k])))\n",
    "\n",
    "num_books = len(texts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(txt):\n",
    "    txt = txt.replace('\\r','')\n",
    "    # txt = txt.replace('\\n',' \\n ')\n",
    "    txt = txt.replace('\\n',' ')\n",
    "    txt = txt.replace(',',' ')\n",
    "    txt = txt.replace(';',' ')\n",
    "    txt = txt.replace('.',' ')\n",
    "    txt = txt.replace('(','')\n",
    "    txt = txt.replace(')','')\n",
    "    txt = txt.replace('!',' ')\n",
    "    txt = txt.replace('?',' ')\n",
    "    txt = txt.replace('_',' ')\n",
    "    txt = txt.replace('“','')\n",
    "    txt = txt.replace('„','')\n",
    "    txt = txt.replace('\"\"','')\n",
    "    txt = txt.lower()\n",
    "    for i in range(5):\n",
    "        txt = txt.replace('  ',' ')\n",
    "    return txt\n",
    "\n",
    "# converts a string to list of words\n",
    "def text2words(dataset_txt):\n",
    "    cleaned_dataset_txt = clean_txt(dataset_txt)\n",
    "    words = cleaned_dataset_txt.rstrip().split(' ')\n",
    "    return words\n",
    "\n",
    "def to_text(sample):\n",
    "    return ' '.join([idx2word[int(x)] for x in sample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hugo words: 573082 distinct words: 27267\n",
      "dickens words: 32247 distinct words: 5152\n",
      "wells words: 63317 distinct words: 7605\n",
      "kipling words: 54907 distinct words: 5711\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for k in texts.keys():\n",
    "    w = text2words(texts[k])\n",
    "    words.append(w)\n",
    "    print(k,\"words:\",len(w),\"distinct words:\",len(set(w)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the dataset from 'words' to 'integers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words: 573082 not known words: 16270 % of unknown words: 0.028390352514997854\n",
      "words: 32247 not known words: 1540 % of unknown words: 0.047756380438490405\n",
      "words: 63317 not known words: 1158 % of unknown words: 0.018288927144368812\n",
      "words: 54907 not known words: 2135 % of unknown words: 0.038883931010617954\n"
     ]
    }
   ],
   "source": [
    "def words2ints(words):\n",
    "    d = []\n",
    "    for word in words:\n",
    "        if word in word2idx:\n",
    "            d.append(word2idx[word])\n",
    "        else:\n",
    "            #print(word)\n",
    "            d.append(0)\n",
    "    return np.array(d)\n",
    "\n",
    "ibooks = []\n",
    "for w in words:\n",
    "    ibooks.append(words2ints(w))\n",
    "\n",
    "for dint in ibooks:\n",
    "    l = len(dint)\n",
    "    l0 = np.count_nonzero(dint==0)\n",
    "    print(\"words:\",l,\"not known words:\",l0, \"% of unknown words:\", l0/l)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: (596, 200, 1)\n",
      "Labels: (596, 4)\n"
     ]
    }
   ],
   "source": [
    "# take MAX word\n",
    "MAX=30000\n",
    "# length of one sample\n",
    "LEN=200\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "for b,book in enumerate(ibooks):\n",
    "    for i in range(0,MAX-LEN,LEN):\n",
    "        samples.append(book[i:LEN+i])\n",
    "        labels.append(b)\n",
    "samples = np.array(samples,dtype=float)\n",
    "samples = np.expand_dims(samples,axis=2)\n",
    "labels = np.array(labels,dtype=float)\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "print(\"Samples:\",samples.shape)\n",
    "print(\"Labels:\",labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace word with the embedding (vector of 50 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with embedding (596, 200, 50)\n"
     ]
    }
   ],
   "source": [
    "samples50 = np.zeros((len(samples),LEN,50))\n",
    "for i in range(len(samples)):\n",
    "    for j in range(LEN):\n",
    "        si = samples[i,j,0]\n",
    "        v = word_lookup[idx2word[int(si)]]\n",
    "        samples50[i,j] = v\n",
    "        \n",
    "print(\"Samples with embedding\",samples50.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 1024)              4202496   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,206,596\n",
      "Trainable params: 4,206,596\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1024)              4403200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 4,407,300\n",
      "Trainable params: 4,407,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(size):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(1024, input_shape=(None,size))) #size is the size of ONE sample\n",
    "    model.add(Dense(num_books,activation='softmax'))\n",
    "    return model  \n",
    "        \n",
    "model1 = build_model(1)\n",
    "model50 = build_model(50)\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model50.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model1.summary()\n",
    "model50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection\n",
    "(trainSamples, testSamples, trainSamples50, testSamples50, trainLabels, testLabels) = sklearn.model_selection.train_test_split(samples, samples50, labels, test_size=0.5, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model without GloVo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 298 samples\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 42s 141ms/sample - loss: 1.8174 - accuracy: 0.2584 - val_loss: 1.5846 - val_accuracy: 0.2148\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 43s 146ms/sample - loss: 1.4584 - accuracy: 0.2685 - val_loss: 1.4775 - val_accuracy: 0.2349\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 1.3641 - accuracy: 0.3255 - val_loss: 1.4923 - val_accuracy: 0.2282\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 54s 181ms/sample - loss: 1.3504 - accuracy: 0.3255 - val_loss: 1.4566 - val_accuracy: 0.2416\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 52s 174ms/sample - loss: 1.3670 - accuracy: 0.3255 - val_loss: 1.5026 - val_accuracy: 0.2315\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 53s 177ms/sample - loss: 1.3396 - accuracy: 0.3221 - val_loss: 1.4710 - val_accuracy: 0.2483\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 54s 182ms/sample - loss: 1.3074 - accuracy: 0.3725 - val_loss: 1.4775 - val_accuracy: 0.2450\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 52s 175ms/sample - loss: 1.3136 - accuracy: 0.3523 - val_loss: 1.4719 - val_accuracy: 0.2752\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.2648 - accuracy: 0.3725 - val_loss: 1.5247 - val_accuracy: 0.2517\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 49s 164ms/sample - loss: 1.2687 - accuracy: 0.3893 - val_loss: 1.5549 - val_accuracy: 0.2383\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "H = model1.fit(trainSamples,trainLabels,epochs=EPOCHS,verbose=1,validation_data=(testSamples,testLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model using GloVo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 298 samples, validate on 298 samples\n",
      "Epoch 1/10\n",
      "298/298 [==============================] - 49s 164ms/sample - loss: 1.4462 - accuracy: 0.3591 - val_loss: 1.3131 - val_accuracy: 0.4060\n",
      "Epoch 2/10\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.2310 - accuracy: 0.4966 - val_loss: 1.0559 - val_accuracy: 0.5705\n",
      "Epoch 3/10\n",
      "298/298 [==============================] - 53s 177ms/sample - loss: 1.4244 - accuracy: 0.4564 - val_loss: 1.3200 - val_accuracy: 0.3557\n",
      "Epoch 4/10\n",
      "298/298 [==============================] - 53s 179ms/sample - loss: 1.2146 - accuracy: 0.5436 - val_loss: 1.2568 - val_accuracy: 0.4362\n",
      "Epoch 5/10\n",
      "298/298 [==============================] - 48s 163ms/sample - loss: 1.4229 - accuracy: 0.5604 - val_loss: 1.2906 - val_accuracy: 0.3591\n",
      "Epoch 6/10\n",
      "298/298 [==============================] - 49s 166ms/sample - loss: 1.1913 - accuracy: 0.5000 - val_loss: 1.2932 - val_accuracy: 0.3658\n",
      "Epoch 7/10\n",
      "298/298 [==============================] - 50s 166ms/sample - loss: 1.1590 - accuracy: 0.5503 - val_loss: 1.2085 - val_accuracy: 0.4463\n",
      "Epoch 8/10\n",
      "298/298 [==============================] - 50s 167ms/sample - loss: 1.1143 - accuracy: 0.5772 - val_loss: 1.1942 - val_accuracy: 0.4698\n",
      "Epoch 9/10\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.1243 - accuracy: 0.5470 - val_loss: 1.3608 - val_accuracy: 0.3557\n",
      "Epoch 10/10\n",
      "298/298 [==============================] - 49s 165ms/sample - loss: 1.1995 - accuracy: 0.4396 - val_loss: 1.2816 - val_accuracy: 0.4497\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "H = model50.fit(trainSamples50,trainLabels,epochs=EPOCHS,verbose=1,validation_data=(testSamples50,testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,cohen_kappa_score\n",
    "def show_results(model,testSamples,testLabels):\n",
    "    testResults = model.predict(testSamples)\n",
    "    print(confusion_matrix(testLabels.argmax(axis=1), testResults.argmax(axis=1)))\n",
    "    #print(classification_report(testLabels.argmax(axis=1), testResults.argmax(axis=1),labels=book_names))\n",
    "    print(classification_report(testLabels.argmax(axis=1), testResults.argmax(axis=1)))\n",
    "    print(\"Cohen's Kappa: {}\".format(cohen_kappa_score(testLabels.argmax(axis=1), testResults.argmax(axis=1))))\n",
    "    print(\"Accuracy: \",accuracy_score(testLabels.argmax(axis=1), testResults.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'book_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-60946710e8be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbook_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mshow_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestSamples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestLabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'book_names' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Model 1\")\n",
    "print(book_names)\n",
    "show_results(model1,testSamples,testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model 50\")\n",
    "print(book_names)\n",
    "show_results(model50,testSamples50,testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model95_30k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_names = [\"hugo\",\"dickens\",\"wells\",\"kipling\"]\n",
    "book_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
