{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2806,
     "status": "ok",
     "timestamp": 1611911484582,
     "user": {
      "displayName": "Paweł Kasprowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gid3j9YiV-A8q1cbwE_iu5-ZeO_Iy_eWmftCVBF=s64",
      "userId": "04465335559857235485"
     },
     "user_tz": -60
    },
    "id": "Ysiaxy0zzgvh"
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU, Reshape, Conv2DTranspose, UpSampling2D, Embedding, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1611911489408,
     "user": {
      "displayName": "Paweł Kasprowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gid3j9YiV-A8q1cbwE_iu5-ZeO_Iy_eWmftCVBF=s64",
      "userId": "04465335559857235485"
     },
     "user_tz": -60
    },
    "id": "JKTEP7nSzgvy"
   },
   "outputs": [],
   "source": [
    "noise_dim = 100 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "def load_img(indir):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    for class_dir in os.listdir(indir):\n",
    "        the_class = class_dir\n",
    "        print(the_class)\n",
    "        for file in os.listdir(indir+'/'+class_dir):\n",
    "            #print(file)\n",
    "            #print(\"{}/{}/{}\".format(indir,class_dir,file))\n",
    "            if file.endswith('ppm'):\n",
    "                image = cv2.imread(\"{}/{}/{}\".format(indir,class_dir,file))\n",
    "                image = cv2.resize(image, (64,64))\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                #image = np.expand_dims(image,axis=2)\n",
    "                samples.append(image)\n",
    "                labels.append(the_class)\n",
    "    samples = np.array(samples)\n",
    "    labels = np.array(labels)\n",
    "    return samples,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 631,
     "status": "ok",
     "timestamp": 1611913231056,
     "user": {
      "displayName": "Paweł Kasprowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gid3j9YiV-A8q1cbwE_iu5-ZeO_Iy_eWmftCVBF=s64",
      "userId": "04465335559857235485"
     },
     "user_tz": -60
    },
    "id": "E4TknOlYzgv0"
   },
   "outputs": [],
   "source": [
    "#from helper import load_img\n",
    "samples, labels = load_img('signs4')\n",
    "print('loaded',len(samples),' samples')\n",
    "print('classes',set(labels))\n",
    "samples = samples/255\n",
    "print(samples.shape)\n",
    "encoder = sklearn.preprocessing.LabelEncoder() # encoder\n",
    "labels = encoder.fit_transform(labels)\n",
    "print(labels[:10])\n",
    "#print(intlabels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "executionInfo": {
     "elapsed": 1706,
     "status": "ok",
     "timestamp": 1611914100374,
     "user": {
      "displayName": "Paweł Kasprowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gid3j9YiV-A8q1cbwE_iu5-ZeO_Iy_eWmftCVBF=s64",
      "userId": "04465335559857235485"
     },
     "user_tz": -60
    },
    "id": "TXoUT5pFzgv2",
    "outputId": "5ad00149-6b0e-4afb-a974-dad2bf8f3358"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i in range(12):\n",
    "    ax = plt.subplot(3, 4, i + 1)\n",
    "    r = random.randint(0,samples.shape[0])\n",
    "    plt.imshow(samples[r][:,:,:])\n",
    "    plt.title(labels[r])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 887,
     "status": "ok",
     "timestamp": 1611913298185,
     "user": {
      "displayName": "Paweł Kasprowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gid3j9YiV-A8q1cbwE_iu5-ZeO_Iy_eWmftCVBF=s64",
      "userId": "04465335559857235485"
     },
     "user_tz": -60
    },
    "id": "ptK_SpGAzgv9",
    "outputId": "f3651430-9f70-4602-bd77-467f1779ff73"
   },
   "outputs": [],
   "source": [
    "# Generator gets a noise vector of size noise_dim and generates an image of size (32 x 32 x 1)\n",
    "# Our aim: we want the image to be as similar to real images (generated above) as possible\n",
    "def make_generator_model():\n",
    "    \n",
    "    \n",
    "    input_layer = tf.keras.layers.Input(shape=(noise_dim,))\n",
    "    x = Dense(8*8*64, use_bias=False)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Reshape((8, 8, 64))(x)\n",
    "\n",
    "    \n",
    "    # tu wpiąć generację z klasy = input, embedding(50), reshape 8x8x1 i na koniec concatenate\n",
    "    input_label = tf.keras.layers.Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    y = Embedding(4, 50)(input_label)\n",
    "    # scale up to image dimensions with linear activation\n",
    "    y = Dense(8*8)(y)\n",
    "    y = Reshape((8, 8, 1))(y)\n",
    "    \n",
    "    merged = Concatenate()([x, y])\n",
    " #   merged = y\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', use_bias=False)(merged)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    # output: 8 x 8 x 128\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2), data_format=None, interpolation=\"nearest\")(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "                    \n",
    "    # output: 16 x 16 x 64\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2), data_format=None, interpolation=\"nearest\")(x)\n",
    "    x = Conv2D(32, (3, 3), strides=(1, 1), padding='same', use_bias=False)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    # output: 32 x 32 x 32\n",
    "\n",
    "    x = UpSampling2D(size=(2, 2), data_format=None, interpolation=\"nearest\")(x)\n",
    "    output_layer = Conv2D(3, (3, 3), strides=(1, 1), padding='same', use_bias=False, activation='sigmoid')(x)\n",
    "    # output: 64 x 64 x 3\n",
    "    model = tf.keras.models.Model(inputs=[input_layer,input_label], outputs=output_layer)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "generator = make_generator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 633,
     "status": "ok",
     "timestamp": 1611913302368,
     "user": {
      "displayName": "Paweł Kasprowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gid3j9YiV-A8q1cbwE_iu5-ZeO_Iy_eWmftCVBF=s64",
      "userId": "04465335559857235485"
     },
     "user_tz": -60
    },
    "id": "hh_nX-DMzgv_",
    "outputId": "6c5b2227-3922-49e1-cfcd-fcafaee4b727"
   },
   "outputs": [],
   "source": [
    "# Discriminator gets image of size (32 x 32 x 1) and decides if it is real or fake\n",
    "# The result of the discriminator is used by generator to improve 'faking'\n",
    "\n",
    "def make_discriminator_model():\n",
    "    \n",
    "    \n",
    "    # zacząć od generację z klasy = input, embedding(50), reshape 32x32x1 i na koniec concatenate\n",
    "    input_label = tf.keras.layers.Input(shape=(1,))\n",
    "    # embedding for categorical input\n",
    "    y = Embedding(4, 50)(input_label)\n",
    "    # scale up to image dimensions with linear activation\n",
    "    y = Dense(64*64*1)(y)\n",
    "    y = Reshape((64, 64, 1))(y)\n",
    " \n",
    "    \n",
    "    \n",
    "    input_layer = tf.keras.layers.Input(shape=(64, 64, 3))\n",
    "    \n",
    "    merged = Concatenate()([input_layer, y])\n",
    "    x = Conv2D(64, (8, 8), strides=(2, 2), padding='same', input_shape=[64, 64, 4])(merged)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Conv2D(128, (8, 8), strides=(2, 2), padding='same')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    output_layer = Dense(1,activation='sigmoid')(x)\n",
    "    model = tf.keras.models.Model(inputs=[input_layer,input_label], outputs=output_layer)\n",
    "    #model.add(Dense(1))\n",
    "    # output: one number 0-fake, 1-real\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns randomly choosen n samples\n",
    "\n",
    "def sample_from_dataset(n,samples):\n",
    "    prev_numbers = []\n",
    "    new_samples = []\n",
    "    new_labels = []\n",
    "    while len(new_samples)<n:\n",
    "        number = random.randrange(len(samples))\n",
    "        if number in prev_numbers: continue\n",
    "        prev_numbers.append(number)\n",
    "        new_samples.append(samples[number])\n",
    "        new_labels.append(labels[number])\n",
    "    new_samples = np.array(new_samples,dtype=float)    \n",
    "    new_labels = np.array(new_labels)    \n",
    "    return new_samples,new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-5)\n",
    "#cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1611913319251,
     "user": {
      "displayName": "Paweł Kasprowski",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gid3j9YiV-A8q1cbwE_iu5-ZeO_Iy_eWmftCVBF=s64",
      "userId": "04465335559857235485"
     },
     "user_tz": -60
    },
    "id": "thDAgGKbzgwB"
   },
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "\n",
    "def calc_ok(vct):\n",
    "    ok = 0\n",
    "    for x in vct: \n",
    "        if x>=0.5: \n",
    "            ok+=1 \n",
    "    return ok\n",
    "\n",
    "# The training step\n",
    "\n",
    "history = []\n",
    "##@tf.function\n",
    "def do_step(images, labels):\n",
    "    batch_size = len(images)\n",
    "    images = np.array(images)\n",
    "    labels = np.expand_dims(labels,axis=1)\n",
    "    # create random noise for generator\n",
    "    input_noise = randn(batch_size * noise_dim)\n",
    "    input_noise = input_noise.reshape(batch_size, noise_dim)\n",
    "    input_noise = tf.convert_to_tensor(input_noise)\n",
    "    #noise = tf.random.normal([batch_size, noise_dim])\n",
    "\n",
    "    random_labels = np.random.randint(4, size=batch_size)\n",
    "    random_labels = np.expand_dims(random_labels,axis=1)\n",
    "  #  print(input_noise.shape)\n",
    "  #  print(random_labels.shape)\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # generate fake image using noise\n",
    "        generated_images = generator([input_noise,random_labels], training=True)\n",
    "        # evaluate fake images\n",
    "        fake_output = discriminator([generated_images,random_labels], training=True)\n",
    "        fake_acc = (batch_size-calc_ok(fake_output))/batch_size\n",
    "        # generator want all images to be accepted (output=1)!\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        \n",
    "        # evaluate real images\n",
    "        real_output = discriminator([images,labels], training=True)\n",
    "        real_acc = calc_ok(real_output)/batch_size\n",
    "        # discriminator wants to classify all real images as 1 and fake images as 0\n",
    "        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "        disc_loss = (real_loss + fake_loss)/2 # sum up both losses\n",
    "\n",
    "    # calculate how to change generator to minimze its loss\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables) # step 4. calculate the gradient of the losses\n",
    "    # calculate how to change discriminator to minimze its loss\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # update weights for both networks\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables)) # step 5. Apply the optimizers and update weights\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch\",epoch,'g_loss=',gen_loss.numpy(),'d_loss=',disc_loss.numpy(),\"real_acc=\",real_acc,\"fake_acc=\",fake_acc)\n",
    "    history.append([gen_loss.numpy(),disc_loss.numpy(),real_acc,fake_acc])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IzfTO1NYzgwC"
   },
   "outputs": [],
   "source": [
    "epochs = 40000\n",
    "for epoch in range(epochs):\n",
    "    # take some random samples\n",
    "    new_samples,new_labels = sample_from_dataset(50,samples)\n",
    "    #print(new_samples.shape)\n",
    "    # perform one training step (epoch)\n",
    "    do_step(new_samples, new_labels)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        # show one real image and some fake images generated by generator using noise seed\n",
    "        #display.clear_output(wait=True)\n",
    "        num_examples_to_generate = 6\n",
    "        seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "        random_labels = np.random.randint(4, size=num_examples_to_generate)\n",
    "        random_labels = np.expand_dims(random_labels,axis=1)\n",
    "        predictions = generator([seed,random_labels], training=False)\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        r = random.randrange(len(samples))\n",
    "        plt.subplot(1, num_examples_to_generate+1, 1)\n",
    "        plt.imshow(samples[r, :, :, :])\n",
    "        plt.title(labels[r])\n",
    "        plt.axis('off')\n",
    "        for i in range(predictions.shape[0]):\n",
    "            plt.subplot(1, num_examples_to_generate+1, i+2)\n",
    "            #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "            plt.imshow(predictions[i, :, :, :])\n",
    "            plt.title(\"{}\".format(random_labels[i]))\n",
    "            plt.axis('off')\n",
    "        plt.show()    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nph = np.array(history)\n",
    "\n",
    "plt.plot(nph[:,0], label='g-loss')\n",
    "plt.plot(nph[:,1], label='d-loss')\n",
    "#plt.ylim([0,2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nph = np.array(history)\n",
    "plt.figure(figsize=(15, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "#plt.ylim([0,2])\n",
    "plt.plot(nph[:,0], label='generator-loss')\n",
    "plt.legend()\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.plot(nph[:,1], label='discriminator-loss')\n",
    "#plt.ylim([0,2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nph = np.array(history)\n",
    "\n",
    "plt.plot(nph[:,2], label='acc-real')\n",
    "plt.plot(nph[:,3], label='acc-fake')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMMF1TpS7WJP"
   },
   "outputs": [],
   "source": [
    "#generator.save_weights('generator_color_4c_40k_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator.save('generator_color_4c_40k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator.save_weights('discriminator_4c_80k_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator.save('discriminator_4c_40k.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dim = 100\n",
    "num_examples_to_generate = 36\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "random_labels = np.random.randint(4, size=num_examples_to_generate)\n",
    "random_labels = np.expand_dims(random_labels,axis=1)\n",
    "\n",
    "print(seed.shape)\n",
    "print(random_labels.shape)\n",
    "predictions = generator([seed,random_labels])\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "# r = random.randrange(len(samples))\n",
    "# plt.subplot(1, num_examples_to_generate+1, 1)\n",
    "#plt.imshow(samples[r, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "#plt.axis('off')\n",
    "for i in range(predictions.shape[0]):\n",
    "    plt.subplot(6, 6, i+1)\n",
    "    #plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "    plt.imshow(predictions[i, :, :, :])\n",
    "    plt.title(\"{}\".format(random_labels[i]))\n",
    "    plt.axis('off')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gan3.ipynb",
   "provenance": [
    {
     "file_id": "1WyIHIOLiZVhkgNmWeLhNl-_C1FJMABmY",
     "timestamp": 1611867870938
    },
    {
     "file_id": "1xZD1vSjIygVakySI8U4UUw-0OLq_8rIR",
     "timestamp": 1611856253043
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
