{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images using loader.py and perform CNN classification\n",
    "\n",
    "@author: pawel@kasprowski.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import loader\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape = (611, 108, 192)\n",
      "shape = (611, 108, 192, 1)\n",
      "inputShape = (108, 192, 1)\n"
     ]
    }
   ],
   "source": [
    "samples,labels,_ = loader.load_img(\"radio_img\")\n",
    "\n",
    "print(\"shape = {}\".format(samples.shape))\n",
    "\n",
    "#add the fourth dimension (color)\n",
    "samples = np.expand_dims(samples, axis=3)\n",
    "\n",
    "print(\"shape = {}\".format(samples.shape))\n",
    "inputShape = (samples.shape[1],samples.shape[2],samples.shape[3])\n",
    "print(\"inputShape = {}\".format(inputShape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights {0: 1.64247311827957, 1: 1.1252302025782688, 2: 0.6655773420479303}\n",
      "Classes: 3\n"
     ]
    }
   ],
   "source": [
    "#weights\n",
    "class_weights = class_weight.compute_class_weight('balanced',classes=np.unique(labels),y=labels)\n",
    "d_class_weights = dict(enumerate(class_weights))\n",
    "print(\"weights {}\".format(d_class_weights))\n",
    "\n",
    "#one-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels)\n",
    "classesNum= labels.shape[1]\n",
    "print (\"Classes: {}\".format(classesNum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and compile the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(inputShape,numClasses):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), padding=\"same\",input_shape=inputShape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(numClasses))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    loss='categorical_crossentropy'    \n",
    "    model.compile(loss=loss, optimizer=\"adam\",metrics=['accuracy'])\n",
    "    return model\n",
    "model = cnn_model(inputShape,classesNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split to training and test\n",
    "(trainSamples, testSamples, trainLabels, testLabels) = train_test_split(samples, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9978\n",
      "Epoch 00001: val_loss improved from inf to 1.03415, saving model to model.01-1.03.h5\n",
      "10/10 [==============================] - 3s 346ms/step - loss: 0.0191 - accuracy: 0.9978 - val_loss: 1.0342 - val_accuracy: 0.6732\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0166 - accuracy: 0.9978\n",
      "Epoch 00002: val_loss did not improve from 1.03415\n",
      "10/10 [==============================] - 3s 327ms/step - loss: 0.0166 - accuracy: 0.9978 - val_loss: 1.0532 - val_accuracy: 0.6732\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss did not improve from 1.03415\n",
      "10/10 [==============================] - 3s 343ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 1.0852 - val_accuracy: 0.6863\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 00004: val_loss did not improve from 1.03415\n",
      "10/10 [==============================] - 4s 351ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 1.1092 - val_accuracy: 0.6797\n",
      "Epoch 00004: early stopping\n",
      "[[28  1  7]\n",
      " [ 3 26 22]\n",
      " [ 2 14 50]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.78      0.81        36\n",
      "           1       0.63      0.51      0.57        51\n",
      "           2       0.63      0.76      0.69        66\n",
      "\n",
      "    accuracy                           0.68       153\n",
      "   macro avg       0.71      0.68      0.69       153\n",
      "weighted avg       0.68      0.68      0.68       153\n",
      "\n",
      "Accuracy CNN: 0.68\n",
      "Cohen's Kappa 0.50\n"
     ]
    }
   ],
   "source": [
    "## callbacks\n",
    "callback1 = ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5', save_best_only=True, verbose=1)\n",
    "callback2 = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "\n",
    "EPOCHS=20\n",
    "BATCH=50\n",
    "model.fit(trainSamples, trainLabels, batch_size=BATCH, epochs=EPOCHS,class_weight=d_class_weights,verbose=1,\n",
    "              callbacks = [callback1,callback2],\n",
    "              validation_data=(testSamples,testLabels))\n",
    "    \n",
    "cnnResults = model.predict(testSamples)\n",
    "\n",
    "print(confusion_matrix(testLabels.argmax(axis=1), cnnResults.argmax(axis=1)))\n",
    "print(classification_report(testLabels.argmax(axis=1), cnnResults.argmax(axis=1)))\n",
    "cnnAcc = accuracy_score(testLabels.argmax(axis=1), cnnResults.argmax(axis=1))\n",
    "print(\"Accuracy CNN: {:.2f}\".format(cnnAcc))\n",
    "print(\"Cohen's Kappa {:.2f}\".format(cohen_kappa_score(testLabels.argmax(axis=1), cnnResults.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
